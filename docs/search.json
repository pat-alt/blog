[
  {
    "objectID": "posts/effortsless-bayesian-dl/index.html#why-bayes",
    "href": "posts/effortsless-bayesian-dl/index.html#why-bayes",
    "title": "Effortless Bayesian Deep Learning in Julia",
    "section": "Why Bayes?",
    "text": "Why Bayes?\nDeep learning has dominated AI research in recent years1 - but how much promise does it really hold? That is very much an ongoing and increasingly polarising debate that you can follow live on Twitter. On one side you have optimists like Ilya Sutskever, chief scientist of OpenAI, who believes that large deep neural networks may already be slighty conscious - that‚Äôs ‚Äúmay‚Äù and ‚Äúslightly‚Äù and only if you just go deep enough? On the other side you have prominent sceptics like Judea Pearl who has long since argued that deep learning still boils down to curve fitting - purely associational and not even remotely intelligent (Pearl and Mackenzie 2018).\nWhatever side of this entertaining debate you find yourself on, the reality is that deep-learning systems have already been deployed at large scale both in academia and industry. More pressing debates therefore revolve around the trustworthiness of these existing systems. How robust are they and in what way exactly do they arrive at decisions that affect each and every one of us? Robustifying deep neural networks generally involves some form of adversarial training, which is costly, can hurt generalization (Raghunathan et al. 2019) and does ultimately not guarantee stability (Bastounis, Hansen, and Vlaƒçiƒá 2021). With respect to interpretability, surrogate explainers like LIME and SHAP are among the most popular tools, but they too have been shown to lack robustness (Slack et al. 2020).\nExactly why are deep neural networks instable and intransparent? Let \\(\\mathcal{D}=\\{x,y\\}_{n=1}^N\\) denote our feature-label pairs and let \\(f(x;\\theta)=y\\) denote some deep neural network specified by its parameters \\(\\theta\\). Then the first thing to note is that the number of free parameters \\(\\theta\\) is typically huge (if you ask Mr Sutskever it really probably cannot be huge enough!). That alone makes it very hard to monitor and interpret the inner workings of deep-learning algorithms. Perhaps more importantly though, the number of parameters relative to the size of \\(\\mathcal{D}\\) is generally huge:\n\n[‚Ä¶] deep neural networks are typically very underspecified by the available data, and [‚Ä¶] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. (Wilson 2020)\n\nIn other words, training a single deep neural network may (and usually does) lead to one random parameter specification that fits the underlying data very well. But in all likelihood there are many other specifications that also fit the data very well. This is both a strength and vulnerability of deep learning: it is a strength because it typically allows us to find one such ‚Äúcompelling explanation‚Äù for the data with ease through stochastic optimization; it is a vulnerability because one has to wonder:\n\nHow compelling is an explanation really if it competes with many other equally compelling, but potentially very different explanations?\n\nA scenario like this very much calls for treating predictions from deep learning models probabilistically [Wilson (2020)]23.\nFormally, we are interested in estimating the posterior predictive distribution as the following Bayesian model average (BMA):\n\\[\np(y|x,\\mathcal{D}) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D})d\\theta\n\\]\nThe integral implies that we essentially need many predictions from many different specifications of \\(\\theta\\). Unfortunately, this means more work for us or rather our computers. Fortunately though, researchers have proposed many ingenious ways to approximate the equation above: Gal and Ghahramani (2016) propose using dropout at test time while Lakshminarayanan, Pritzel, and Blundell (2016) show that averaging over an ensemble of just five models seems to do the trick. Still, despite their simplicity and usefulness these approaches involve additional computational costs compared to training just a single network. As we shall see now though, another promising approach has recently entered the limelight: Laplace approximation (LA).\nIf you have read my previous post on Bayesian Logisitic Regression, then the term Laplace should already sound familiar to you. As a matter of fact, we will see that all concepts covered in that previous post can be naturally extended to deep learning. While some of these concepts will be revisited below, I strongly recommend you check out the previous post before reading on here. Without further ado let us now see how LA can be used for truly effortless deep learning."
  },
  {
    "objectID": "posts/effortsless-bayesian-dl/index.html#laplace-approximation",
    "href": "posts/effortsless-bayesian-dl/index.html#laplace-approximation",
    "title": "Go deep, but also ‚Ä¶ go Bayesian!",
    "section": "Laplace Approximation",
    "text": "Laplace Approximation\nWhile LA was first proposed in the 18th century, it has so far not attracted serious attention from the deep learning community largely because it involves a possibly large Hessian computation. Daxberger et al. (2021) are on a mission to change the perception that LA has no use in DL: in their NeurIPS 2021 paper they demonstrate empirically that LA can be used to produce Bayesian model averages that are at least at par with existing approaches in terms of uncertainty quantification and out-of-distribution detection. They show that recent advancements in autodifferentation can be leveraged to produce fast and accurate approximations of the Hessian and even provide a fully-fledged Python library that can be used with any pretrained Torch model. For this post, I have built a much less comprehensive, pure-play equivalent of their package in Julia - BayesLaplace.jl can be used with deep learning models built in Flux.jl, which is Julia‚Äôs main DL library. As in the previous post on Bayesian logistic regression I will rely on Julia code snippits instead of equations to convey the underlying maths. If you‚Äôre curious about the maths, the NeurIPS 2021 paper provides all the detail you need.\n\nFrom Bayesian Logistic Regression ‚Ä¶\nLet‚Äôs recap: in the case of logistic regression we had a assumed a zero-mean Gaussian prior \\(p(\\mathbf{w}) \\sim \\mathcal{N} \\left( \\mathbf{w} | \\mathbf{0}, \\sigma_0^2 \\mathbf{I} \\right)=\\mathcal{N} \\left( \\mathbf{w} | \\mathbf{0}, \\mathbf{H}_0^{-1} \\right)\\) for the weights that are used to compute logits \\(\\mu_n=\\mathbf{w}^T\\mathbf{x}_n\\), which in turn are fed to a sigmoid function to produce probabilities \\(p(y_n=1)=\\sigma(\\mu_n)\\). We saw that under this assumption solving the logistic regression problem corresponds to minimizing the following differentiable loss function:\n\\[\n\\ell(\\mathbf{w})= - \\sum_{n}^N [y_n \\log \\mu_n + (1-y_n)\\log (1-\\mu_n)] + \\frac{1}{2} (\\mathbf{w}-\\mathbf{w}_0)^T\\mathbf{H}_0(\\mathbf{w}-\\mathbf{w}_0)\n\\]\nAs our first step towards Bayesian deep learning, we observe the following: the loss function above corresponds to the objective faced by a single-layer artificial neural network with sigmoid activation and weight decay4. In other words, regularized logistic regression is equivalent to a very simple neural network architecture and hence it is not surprising that underlying concepts can in theory be applied in much the same way.\nSo let‚Äôs quickly recap the next core concept: LA relies on the fact that the second-order Taylor expansion of our loss function \\(\\ell\\) evaluated at the maximum a posteriori (MAP) estimate \\(\\mathbf{\\hat{w}}=\\arg\\max_{\\mathbf{w}} p(\\mathbf{w}|\\mathcal{D})\\) amounts to a multi-variate Gaussian distribution. In particular, that Gaussian is centered around the MAP estimate with covariance equal to the inverse Hessian evaluated at the mode \\(\\hat{\\Sigma}=(\\mathbf{H}(\\mathbf{\\hat{w}}))^{-1}\\) (Murphy 2022).\nThat is basically all there is to the story: if we have a good estimate of \\(\\mathbf{H}(\\mathbf{\\hat{w}})\\) we have an analytical expression for an (approximate) posterior over parameters. So let‚Äôs go ahead and start by run Bayesian Logistic regression using Flux.jl. We begin by loading some required packages including BayesLaplace.jl. It ships with a helper function toy_data_linear that creates a toy data set composed of linearly separable samples evenly balanced across the two classes.\n\n# Import libraries.\nusing Flux, Plots, Random, PlotThemes, Statistics, BayesLaplace\ntheme(:juno)\n# Number of points to generate.\nxs, y = toy_data_linear(100)\nX = hcat(xs...); # bring into tabular format\ndata = zip(xs,y);\n\nThen we proceed to prepare the single-layer neural network with weight decay. The term \\(\\lambda\\) determines the strength of the \\(\\ell2\\) penalty: we regularize parameters \\(\\theta\\) more heavily for higher values. Equivalently, we can say that from the Bayesian perspective it governs the strength of the prior \\(p(\\theta) \\sim \\mathcal{N} \\left( \\theta | \\mathbf{0}, \\sigma_0^2 \\mathbf{I} \\right)= \\mathcal{N} \\left( \\mathbf{w} | \\mathbf{0}, \\lambda_0^{-2} \\mathbf{I} \\right)\\): a higher value of \\(\\lambda\\) indicates a higher conviction about our prior belief that \\(\\theta=\\mathbf{0}\\), which is of course equivalent to regularizing more heavily. The exact choice of \\(\\lambda=0.5\\) for this toy example is somewhat arbitrary (it made for good visualizations below). Note that I have used \\(\\theta\\) to denote our neural parameters to distinguish the case from Bayesian logistic regression, but we are in fact still solving the same problem.\n\nnn = Chain(Dense(2,1))\nŒª = 0.5\nsqnorm(x) = sum(abs2, x)\nweight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization();\n\nBefore we apply Laplace approximation we train our model:\n\nusing Flux.Optimise: update!, ADAM\nopt = ADAM()\nepochs = 50\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, params(nn), gs)\n  end\nend\n\nUp until this point we have just followed the standard recipe for training a regularized artificial neural network in Flux.jl for a simple binary classification task. To compute the Laplace approximation using BayesLaplace.jl we need just two more lines of code:\n\nla = laplace(nn, Œª=Œª)\nfit!(la, data);\n\nUnder the hood the Hessian is approximated through the empirical Fisher, which can be computed using only the gradients of our loss function \\(\\nabla_{\\theta}\\ell(f(\\mathbf{x}_n;\\theta,y_n))\\) where \\(\\{\\mathbf{x}_n,y_n\\}\\) are training data (see NeurIPS 2021 paper for details). Finally, BayesLaplace.jl ships with a function predict(ùë≥::LaplaceRedux, X::AbstractArray; link_approx=:probit) that computes the posterior predictive using a probit approximation, much like we saw in the previous post. That function is used under the hood of the plot_contour function below to create the right panel of Figure¬†1. It visualizes the posterior predictive distribution in the 2D feature space. For comparison I have added the corresponding plugin estimate as well. Note how for the Laplace approximation the predicted probabilities fan out indicating that confidence decrease in regions scarce of data.\n\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin);\np_laplace = plot_contour(X',y,la;title=\"Laplace\")\n# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\nsavefig(plt, \"www/posterior_predictive_logit.png\");\n\n\n\n\nFigure 1: Posterior predictive distribution of Logistic regression in the 2D feature space using plugin estimator (left) and Laplace approximation (right).\n\n\n\n\n‚Ä¶ to Bayesian Neural Networks\nNow let‚Äôs step it up a notch: we will repeat the exercise from above, but this time for data that is not linearly separable using a simple MLP instead of the single-layer neural network we used above. The code below is almost the same as above, so I will not go through the various steps again.\n\n# Number of points to generate:\nxs, y = toy_data_non_linear(200)\nX = hcat(xs...); # bring into tabular format\ndata = zip(xs,y)\n\n# Build MLP:\nn_hidden = 32\nD = size(X)[1]\nnn = Chain(\n    Dense(D, n_hidden, œÉ),\n    Dense(n_hidden, 1)\n)  \nŒª = 0.01\nsqnorm(x) = sum(abs2, x)\nweight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization()\n\n# Training:\nepochs = 200\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, params(nn), gs)\n  end\nend\n\nFitting the Laplace approximation is also analogous, but note that this we have added an argument: subset_of_weights=:last_layer. This specifies that we only want to use the parameters of the last layer of our MLP. While we could have used all of them (subset_of_weights=:all), Daxberger et al. (2021) find that the last-layer Laplace approximation produces satisfying results, while be computationally cheaper. Figure¬†2 demonstrates that once again the Laplace approximation yields a posterior predictive distribution that is more conservative than the over-confident plugin estimate.\n\nla = laplace(nn, Œª=Œª, subset_of_weights=:last_layer)\nfit!(la, data);\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin)\np_laplace = plot_contour(X',y,la;title=\"Laplace\")\n# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\nsavefig(plt, \"www/posterior_predictive_mlp.png\");\n\n\n\n\nFigure 2: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right).\n\n\nTo see why this is a desirable outcome consider the zoomed out version of Figure¬†2 below: the plugin estimator classifies with full confidence in regions completely scarce of any data. Arguably Laplace approximation produces a much more reasonable picture, even though it too could likely be improved by fine-tuning our choice of \\(\\lambda\\) and the neural network architecture.\n\nzoom=-50\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin,zoom=zoom);\np_laplace = plot_contour(X',y,la;title=\"Laplace\",zoom=zoom);\n# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400));\nsavefig(plt, \"www/posterior_predictive_mlp_zoom.png\");\n\n\n\n\nFigure 3: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right). Zoomed out."
  },
  {
    "objectID": "posts/effortsless-bayesian-dl/index.html#wrapping-up",
    "href": "posts/effortsless-bayesian-dl/index.html#wrapping-up",
    "title": "Go deep, but also ‚Ä¶ go Bayesian!",
    "section": "Wrapping up",
    "text": "Wrapping up\nRecent state-of-the-art research on neural information processing suggests that Bayesian deep learning can be effortless: Laplace approximation for deep neural networks appears to work very well and it does so at minimal computational cost (Daxberger et al. 2021). This is great news, because the case for turning Bayesian is strong: society increasingly relies on complex automated decision-making systems that need to be trustworthy. More and more of these systems involve deep learning which in and of itself is not trustworthy. We have seen that typically there exist various viable parameterizations of deep neural networks each with their own distinct and compelling explanation for the data at hand. When faced with many viable options, don‚Äôt put all of your eggs in one basket. In other words, go Bayesian!"
  },
  {
    "objectID": "posts/effortsless-bayesian-dl/index.html#references",
    "href": "posts/effortsless-bayesian-dl/index.html#references",
    "title": "Go deep, but also ‚Ä¶ go Bayesian!",
    "section": "References",
    "text": "References\n\n\n\n\nBastounis, Alexander, Anders C Hansen, and Verner Vlaƒçiƒá. 2021. ‚ÄúThe Mathematics of Adversarial Attacks in AI‚ÄìWhy Deep Learning Is Unstable Despite the Existence of Stable Neural Networks.‚Äù arXiv Preprint arXiv:2109.06098.\n\n\nDaxberger, Erik, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. 2021. ‚ÄúLaplace Redux-Effortless Bayesian Deep Learning.‚Äù Advances in Neural Information Processing Systems 34.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. ‚ÄúDropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.‚Äù In International Conference on Machine Learning, 1050‚Äì59. PMLR.\n\n\nLakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2016. ‚ÄúSimple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles.‚Äù arXiv Preprint arXiv:1612.01474.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. MIT Press.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. Basic books.\n\n\nRaghunathan, Aditi, Sang Michael Xie, Fanny Yang, John C Duchi, and Percy Liang. 2019. ‚ÄúAdversarial Training Can Hurt Generalization.‚Äù arXiv Preprint arXiv:1906.06032.\n\n\nSlack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. ‚ÄúFooling Lime and Shap: Adversarial Attacks on Post Hoc Explanation Methods.‚Äù In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 180‚Äì86.\n\n\nWilson, Andrew Gordon. 2020. ‚ÄúThe Case for Bayesian Deep Learning.‚Äù arXiv Preprint arXiv:2001.10995."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Newest\n        \n         \n          Date - Oldset\n        \n         \n          Author\n        \n         \n          Modified - Newest\n        \n         \n          Modified - Oldset\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nGo deep, but also ‚Ä¶ go Bayesian!\n\n\nAn introduction to effortless Bayesian deep learning through Laplace approximation coded from scratch in Julia. See also the pure-play companion package BayesLaplace.jl\n\n\n\n\n\n\n18/02/2022\n\n\nPatrick Altmeyer\n\n\n9 min\n\n\n19/02/2022, 09:09:10\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Logistic Regression\n\n\nAn introduction to Bayesian Logistic Regression from the bottom up with examples in Julia language.\n\n\n\n\n\n\n15/11/2021\n\n\nPatrick Altmeyer\n\n\n13 min\n\n\n19/02/2022, 09:08:39\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/effortsless-bayesian-dl/index.html#go-deep-but-also-go-bayesian",
    "href": "posts/effortsless-bayesian-dl/index.html#go-deep-but-also-go-bayesian",
    "title": "Go deep, but also ‚Ä¶ go Bayesian!",
    "section": "Go deep, but also ‚Ä¶ go Bayesian!",
    "text": "Go deep, but also ‚Ä¶ go Bayesian!\nDeep learning has dominated AI research in recent years1 - but how much promise does it really hold? That is very much an ongoing and increasingly polarising debate that you can follow live on Twitter. On one side you have optimists like Ilya Sutskever, chief scientist of OpenAI, who believes that large deep neural networks may already be slighty conscious - that‚Äôs ‚Äúmay‚Äù and ‚Äúslightly‚Äù and only if you just go deep enough? On the other side you have prominent sceptics like Judea Pearl who has long since argued that deep learning still boils down to curve fitting - purely associational and not even remotely intelligent (Pearl and Mackenzie 2018).\nWhatever side of this entertaining debate you find yourself on, the reality is that deep-learning systems have already been deployed at large scale both in academia and industry. More pressing debates therefore revolve around the trustworthiness of these existing systems. How robust are they and in what way exactly do they arrive at decisions that affect each and every one of us? Robustifying deep neural networks generally involves some form of adversarial training, which is costly, can hurt generalization (Raghunathan et al. 2019) and does ultimately not guarantee stability (Bastounis, Hansen, and Vlaƒçiƒá 2021). With respect to interpretability, surrogate explainers like LIME and SHAP are among the most popular tools, but they too have been shown to lack robustness (Slack et al. 2020).\nExactly why are deep neural networks instable and intransparent? Let \\(\\mathcal{D}=\\{x,y\\}_{n=1}^N\\) denote our feature-label pairs and let \\(f(x;\\theta)=y\\) denote some deep neural network specified by its parameters \\(\\theta\\). Then the first thing to note is that the number of free parameters \\(\\theta\\) is typically huge (if you ask Mr Sutskever it really probably cannot be huge enough!). That alone makes it very hard to monitor and interpret the inner workings of deep-learning algorithms. Perhaps more importantly though, the number of parameters relative to the size of \\(\\mathcal{D}\\) is generally huge:\n\n[‚Ä¶] deep neural networks are typically very underspecified by the available data, and [‚Ä¶] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. (Wilson 2020)\n\nIn other words, training a single deep neural network may (and usually does) lead to one random parameter specification that fits the underlying data very well. But in all likelihood there are many other specifications that also fit the data very well. This is both a strength and vulnerability of deep learning: it is a strength because it typically allows us to find one such ‚Äúcompelling explanation‚Äù for the data with ease through stochastic optimization; it is a vulnerability because one has to wonder:\n\nHow compelling is an explanation really if it competes with many other equally compelling, but potentially very different explanations?\n\nA scenario like this very much calls for treating predictions from deep learning models probabilistically [Wilson (2020)]23.\nFormally, we are interested in estimating the posterior predictive distribution as the following Bayesian model average (BMA):\n\\[\np(y|x,\\mathcal{D}) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D})d\\theta\n\\]\nThe integral implies that we essentially need many predictions from many different specifications of \\(\\theta\\). Unfortunately, this means more work for us or rather our computers. Fortunately though, researchers have proposed many ingenious ways to approximate the equation above: Gal and Ghahramani (2016) propose using dropout at test time while Lakshminarayanan, Pritzel, and Blundell (2016) show that averaging over an ensemble of just five models seems to do the trick. Still, despite their simplicity and usefulness these approaches involve additional computational costs compared to training just a single network. As we shall see now though, another promising approach has recently entered the limelight: Laplace approximation (LA).\nIf you have read my previous post on Bayesian Logisitic Regression, then the term Laplace should already sound familiar to you. As a matter of fact, we will see that all concepts covered in that previous post can be naturally extended to deep learning. While some of these concepts will be revisited below, I strongly recommend you check out the previous post before reading on here. Without further ado let us now see how LA can be used for truly effortless deep learning."
  },
  {
    "objectID": "posts/effortsless-bayesian-dl/index.html#the-case-for-bayesian-deep-learning",
    "href": "posts/effortsless-bayesian-dl/index.html#the-case-for-bayesian-deep-learning",
    "title": "Go deep, but also ‚Ä¶ go Bayesian!",
    "section": "The case for Bayesian Deep Learning",
    "text": "The case for Bayesian Deep Learning\nWhatever side of this entertaining debate you find yourself on, the reality is that deep-learning systems have already been deployed at large scale both in academia and industry. More pressing debates therefore revolve around the trustworthiness of these existing systems. How robust are they and in what way exactly do they arrive at decisions that affect each and every one of us? Robustifying deep neural networks generally involves some form of adversarial training, which is costly, can hurt generalization (Raghunathan et al. 2019) and does ultimately not guarantee stability (Bastounis, Hansen, and Vlaƒçiƒá 2021). With respect to interpretability, surrogate explainers like LIME and SHAP are among the most popular tools, but they too have been shown to lack robustness (Slack et al. 2020).\nExactly why are deep neural networks unstable and in-transparent? Let \\(\\mathcal{D}=\\{x,y\\}_{n=1}^N\\) denote our feature-label pairs and let \\(f(x;\\theta)=y\\) denote some deep neural network specified by its parameters \\(\\theta\\). Then the first thing to note is that the number of free parameters \\(\\theta\\) is typically huge (if you ask Mr Sutskever it really probably cannot be huge enough!). That alone makes it very hard to monitor and interpret the inner workings of deep-learning algorithms. Perhaps more importantly though, the number of parameters relative to the size of \\(\\mathcal{D}\\) is generally huge:\n\n[‚Ä¶] deep neural networks are typically very underspecified by the available data, and [‚Ä¶] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. (Wilson 2020)\n\nIn other words, training a single deep neural network may (and usually does) lead to one random parameter specification that fits the underlying data very well. But in all likelihood there are many other specifications that also fit the data very well. This is both a strength and vulnerability of deep learning: it is a strength because it typically allows us to find one such ‚Äúcompelling explanation‚Äù for the data with ease through stochastic optimization; it is a vulnerability because one has to wonder:\n\nHow compelling is an explanation really if it competes with many other equally compelling, but potentially very different explanations?\n\nA scenario like this very much calls for treating predictions from deep learning models probabilistically [Wilson (2020)]23.\nFormally, we are interested in estimating the posterior predictive distribution as the following Bayesian model average (BMA):\n\\[\np(y|x,\\mathcal{D}) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D})d\\theta\n\\]\nThe integral implies that we essentially need many predictions from many different specifications of \\(\\theta\\). Unfortunately, this means more work for us or rather our computers. Fortunately though, researchers have proposed many ingenious ways to approximate the equation above: Gal and Ghahramani (2016) propose using dropout at test time while Lakshminarayanan, Pritzel, and Blundell (2016) show that averaging over an ensemble of just five models seems to do the trick. Still, despite their simplicity and usefulness these approaches involve additional computational costs compared to training just a single network. As we shall see now though, another promising approach has recently entered the limelight: Laplace approximation (LA).\nIf you have read my previous post on Bayesian Logistic Regression, then the term Laplace should already sound familiar to you. As a matter of fact, we will see that all concepts covered in that previous post can be naturally extended to deep learning. While some of these concepts will be revisited below, I strongly recommend you check out the previous post before reading on here. Without further ado let us now see how LA can be used for truly effortless deep learning."
  },
  {
    "objectID": "posts/effortsless-bayesian-dl/index.html#resources",
    "href": "posts/effortsless-bayesian-dl/index.html#resources",
    "title": "Go deep, but also ‚Ä¶ go Bayesian!",
    "section": "Resources",
    "text": "Resources\nTo get started with Bayesian deep learning I have found many useful and free resources online, some of which are listed below:\n\nTuring.jl tutorial on Bayesian deep learning in Julia\nVarious RStudio AI blog posts including this one and this one\nTensorFlow blog post on regression with probabilistic layers\nKevin Murphy‚Äôs draft text book, now also available as print"
  },
  {
    "objectID": "posts/bayesian-logit/index.html#uncertainty",
    "href": "posts/bayesian-logit/index.html#uncertainty",
    "title": "Bayesian Logistic Regression",
    "section": "Uncertainty",
    "text": "Uncertainty\n\n\n\n\nSimulation of changing parameter distribution.\n\n\n\nIf you‚Äôve ever searched for evaluation metrics to assess model accuracy, chances are that you found many different options to choose from (too many?). Accuracy is in some sense the holy grail of prediction so it‚Äôs not at all surprising that the machine learning community spends a lot time thinking about it. In a world where more and more high-stake decisions are being automated, model accuracy is in fact a very valid concern.\nBut does this recipe for model evaluation seem like a sound and complete approach to automated decision-making? Haven‚Äôt we forgot anything? Some would argue that we need to pay more attention to model uncertainty. No matter how many times you have cross-validated your model, the loss metric that it is being optimized against as well as its parameters and predictions remain inherently random variables. Focusing merely on prediction accuracy and ignoring uncertainty altogether can install a false level of confidence in automated decision-making systems. Any trustworthy approach to learning from data should therefore at the very least be transparent about its own uncertainty.\nHow can we estimate uncertainty around model parameters and predictions? Frequentist methods for uncertainty quantification generally involve either closed-form solutions based on asymptotic theory or bootstrapping (see for example here for the case of logistic regression). In Bayesian statistics and machine learning we are instead concerned with modelling the posterior distribution over model parameters. This approach to uncertainty quantification is known as Bayesian Inference because we treat model parameters in a Bayesian way: we make assumptions about their distribution based on prior knowledge or beliefs and update these beliefs in light of new evidence. The frequentist approach avoids the need for being explicit about prior beliefs, which in the past has sometimes been considered as unscientific. However, frequentist methods come with their own assumptions and pitfalls (see for example Murphy (2012)) for a discussion). Without diving further into this argument, let us now see how Bayesian Logistic Regression can be implemented from the bottom up."
  },
  {
    "objectID": "posts/bayesian-logit/index.html#the-ground-truth",
    "href": "posts/bayesian-logit/index.html#the-ground-truth",
    "title": "Bayesian Logistic Regression",
    "section": "The ground truth",
    "text": "The ground truth\n\n\n\nIn this post we will work with a synthetic toy data set \\(\\mathcal{D}\\) composed of \\(N\\) binary labels \\(y_n\\in\\{0,1\\}\\) and corresponding feature vectors \\(\\mathbf{x}_n\\in \\mathbb{R}^D\\). Working with synthetic data has the benefit that we have control over the ground truth that generates our data. In particular, we will assume that the binary labels \\(y_n\\) are generated by a logistic regression model\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(y_n|\\mathbf{x}_n;\\mathbf{w})&\\sim\\text{Ber}(y_n|\\sigma(\\mathbf{w}^T\\mathbf{x}_n)) \\\\\n\\end{aligned}\n(\\#eq:logreg)\n\\end{equation}\n\\]\nwhere \\(\\sigma(a)=1/(1+e^{-a})\\) is the sigmoid or logit function (Murphy 2022).1 Features are generated from a mixed Gaussian model.\nTo add a little bit of life to our example we will assume that the binary labels classify samples into cats and dogs, based on their height and tail length. Figure @ref(fig:ground) shows the synthetic data in the two-dimensional feature domain. Following an introduction to Bayesian Logistic Regression in the next section we will use the synthetic data \\(\\mathcal{D}\\) to estimate our model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGround truth labels."
  },
  {
    "objectID": "posts/bayesian-logit/index.html#the-maths",
    "href": "posts/bayesian-logit/index.html#the-maths",
    "title": "Bayesian Logistic Regression",
    "section": "The maths",
    "text": "The maths\nEstimation usually boils down to finding the vector of parameters \\(\\hat{\\mathbf{w}}\\) that maximizes the likelihood of observing \\(\\mathcal{D}\\) under the assumed model. That estimate can then be used to compute predictions for some new unlabelled data set \\(\\mathcal{D}=\\{x_m:m=1,...,M\\}\\).\n\nProblem setup\nThe starting point for Bayesian Logistic Regression is Bayes‚Äô Theorem:\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(\\mathbf{w}|\\mathcal{D})&\\propto p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w}) \\\\\n\\end{aligned}\n(\\#eq:posterior)\n\\end{equation}\n\\] Formally, this says that the posterior distribution of parameters \\(\\mathbf{w}\\) is proportional to the product of the likelihood of observing \\(\\mathcal{D}\\) given \\(\\mathbf{w}\\) and the prior density of \\(\\mathbf{w}\\). Applied to our context this can intuitively be understood as follows: our posterior beliefs around \\(\\mathbf{w}\\) are formed by both our prior beliefs and the evidence we observe. Yet another way to look at this is that maximising @ref(eq:posterior) with respect to \\(\\mathbf{w}\\) corresponds to maximum likelihood estimation regularized by prior beliefs (we will come back to this).\nUnder the assumption that individual label-feature pairs are independently and identically distributed, their joint likelihood is simply the product over their individual densities. The prior beliefs around \\(\\mathbf{w}\\) are at our discretion. In practice they may be derived from previous experiments. Here we will use a zero-mean spherical Gaussian prior for reasons explained further below. To sum this up we have\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(\\mathcal{D}|\\mathbf{w})& \\sim \\prod_{n=1}^N p(y_n|\\mathbf{x}_n;\\mathbf{w})\\\\\n&& p(\\mathbf{w})& \\sim \\mathcal{N} \\left( \\mathbf{w} | \\mathbf{w}_0, \\Sigma_0 \\right) \\\\\n\\end{aligned}\n(\\#eq:prior)\n\\end{equation}\n\\]\nwith \\(\\mathbf{w}_0=\\mathbf{0}\\) and \\(\\Sigma_0=\\sigma^2\\mathbf{I}\\). Plugging this into Bayes‚Äô rule we finally have\n\\[\n\\begin{aligned}\n&& p(\\mathbf{w}|\\mathcal{D})&\\propto\\prod_{n=1}^N \\text{Ber}(y_n|\\sigma(\\mathbf{w}^T\\mathbf{x}_n))\\mathcal{N} \\left( \\mathbf{w} | \\mathbf{w}_0, \\Sigma_0 \\right) \\\\\n\\end{aligned}\n\\]\nUnlike with linear regression there are no closed-form analytical solutions to estimating or maximising this posterior, but fortunately accurate approximations do exist (Murphy 2022). One of the simplest approaches called Laplace Approximation is straight-forward to implement and computationally very efficient. It relies on the observation that under the assumption of a Gaussian prior, the posterior of logistic regression is also approximately Gaussian: in particular, this Gaussian distribution is centered around the maximum a posteriori (MAP) estimate \\(\\hat{\\mathbf{w}}=\\arg\\max_{\\mathbf{w}} p(\\mathbf{w}|\\mathcal{D})\\) with a covariance matrix equal to the inverse Hessian evaluated at the mode \\(\\hat{\\Sigma}=(\\mathbf{H}(\\hat{\\mathbf{w}}))^{-1}\\). With that in mind, finding \\(\\hat{\\mathbf{w}}\\) seems like a natural next step.\n\n\nSolving the problem\nIn practice we do not maximize the posterior \\(p(\\mathbf{w}|\\mathcal{D})\\) directly. Instead we minimize the negative log likelihood, which is an equivalent optimization problem and easier to implement. In @ref(eq:likeli) below I have denoted the negative log likelihood as \\(\\ell(\\mathbf{w})\\) indicating that this is the loss function we aim to minimize. The following two lines in @ref(eq:likeli) show the gradient and Hessian - so the first- and second-order derivatives of \\(\\ell\\) with respect to \\(\\mathbf{w}\\) - where \\(\\mathbf{H}_0=\\Sigma_0^{-1}\\) and \\(\\mu_n=\\sigma(\\mathbf{w}^T\\mathbf{x}_n)\\). To understand how exactly the gradient and Hessian are derived see for example chapter 10 in Murphy (2022).2.\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& \\ell(\\mathbf{w})&=- \\sum_{n=1}^{N} [y_n \\log \\mu_n + (1-y_n)\\log (1-\\mu_n)] + \\frac{1}{2} (\\mathbf{w}-\\mathbf{w}_0)^T\\mathbf{H}_0(\\mathbf{w}-\\mathbf{w}_0) \\\\\n&& \\nabla_{\\mathbf{w}}\\ell(\\mathbf{w})&= \\sum_{n=1}^{N} (\\mu_n-y_n) \\mathbf{x}_n + \\mathbf{H}_0(\\mathbf{w}-\\mathbf{w}_0) \\\\\n&& \\nabla^2_{\\mathbf{w}}\\ell(\\mathbf{w})&= \\sum_{n=1}^{N} (\\mu_n-y_n) \\left( \\mu_n(1-\\mu_n) \\mathbf{x}_n \\mathbf{x}_n^T \\right) + \\mathbf{H}_0\\\\\n\\end{aligned}\n(\\#eq:likeli)\n\\end{equation}\n\\]\n\nSIDENOTE üí°\nNote how earlier I mentioned that maximising the posterior likelihood can be seen as regularized maximum likelihood estimation. We can now make that connection explicit: in @ref(eq:likeli) let us assume that \\(\\mathbf{w}_0=\\mathbf{0}\\). Then since \\(\\mathbf{H}_0=\\lambda\\mathbf{I}\\) with \\(1/\\sigma^2\\) the second term in the first line is simply \\(\\lambda \\frac{1}{2} \\mathbf{w}^T\\mathbf{w}=\\lambda \\frac{1}{2} ||\\mathbf{w}||_2^2\\). This is equivalent to running logistic regression with an \\(\\ell_2\\)-penalty (Bishop 2006).\n\n\nSince minimizing the loss function in @ref(eq:likeli) is a convex optimization problem we have many efficient algorithms to choose from in order to solve this problem. With the Hessian at hand it seems natural to use a second-order method, because incorporating information about the curvature of the loss function generally leads to faster convergence. Here we will implement Newton‚Äôs method in line with the presentation in chapter 8 of Murphy (2022).\n\n\nPosterior predictive\nSuppose now that we have trained the Bayesian Logistic Regression model as our binary classifier \\(g_N(\\mathbf{x})\\) using our training data \\(\\mathcal{D}\\). A new unlabelled sample \\((\\mathbf{x}_{N+1},?)\\) arrives. As with any binary classifier we can predict the missing label by simply plugging the new sample into our classifier \\(\\hat{y}_{N+1}=g_N(\\mathbf{x}_{N+1})=\\sigma(\\hat{\\mathbf{w}}^T\\mathbf{x}_{N+1})\\), where \\(\\hat{\\mathbf{w}}\\) is the MAP estimate as before. If at training phase we have found \\(g_N(\\mathbf{x})\\) to achieve good accuracy, we may expect \\((\\mathbf{x}_{N+1},\\hat{y}_{N+1})\\) to be a reasonably good approximation of the true and unobserved pair \\((\\mathbf{x}_{N+1},y_{N+1})\\). But since we are still dealing with an expected value of a random variable, we would generally like to have an idea of how noisy this prediction is.\nFormally, we are interested in the posterior predictive distribution:\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(y=1|\\mathbf{x}, \\mathcal{D})&= \\int \\sigma(\\mathbf{w}^T \\mathbf{x})p(\\mathbf{w}|\\mathcal{D})d\\mathbf{w} \\\\\n\\end{aligned}\n(\\#eq:posterior-pred)\n\\end{equation}\n\\]\n\nSIDENOTE üí°\nThe approach that ignores uncertainty altogether corresponds to what is referred to as plugin approximation of the posterior predictive. Formally, it imposes \\(p(y=1|\\mathbf{x}, \\mathcal{D})\\approx p(y=1|\\mathbf{x}, \\hat{\\mathbf{w}})\\).\n\n\nWith the posterior distribution over model parameters \\(p(\\mathbf{w}|\\mathcal{D})\\) at hand we have the necessary ingredients to estimate the posterior predictive distribution \\(p(y=1|\\mathbf{x}, \\mathcal{D})\\).\nAn obvious, but computationally expensive way to estimate it is through Monte Carlo: draw \\(\\mathbf{w}_s\\) from \\(p(\\mathbf{w}|\\mathcal{D})\\) for \\(s=1:S\\) and compute fitted values \\(\\sigma(\\mathbf{w_s}^T\\mathbf{x})\\) each. Then the posterior predictive distribution corresponds to the average over all fitted values, \\(p(y=1|\\mathbf{x}, \\mathcal{D})=1/S \\sum_{s=1}^{S}\\sigma(\\mathbf{w_s}^T\\mathbf{x})\\). By the law of large numbers the Monte Carlo estimate is an accurate estimate of the true posterior predictive for large enough \\(S\\). Of course, ‚Äúlarge enough‚Äù is somewhat loosely defined here and depending on the problem can mean ‚Äúvery large‚Äù. Consequently, the computational costs involved essentially know no upper bound.\nFortunately, it turns out that we can trade off a little bit of accuracy in return for a convenient analytical solution. In particular, we have that \\(\\sigma(a) \\approx \\Phi(\\lambda a)\\) where \\(\\Phi(.)\\) is the standard Gaussian cdf and \\(\\lambda=\\pi/8\\) ensures that the two functions have the same slope at the origin (Figure @ref(fig:probit)). Without dwelling further on the details we can use this finding to approximate the integral in @ref(eq:posterior-pred) as a sigmoid function. This is called probit approximation and implemented below.\n\n\n\n\n\nDemonstration of the probit approximation."
  },
  {
    "objectID": "posts/bayesian-logit/index.html#the-code",
    "href": "posts/bayesian-logit/index.html#the-code",
    "title": "Bayesian Logistic Regression",
    "section": "The code",
    "text": "The code\nWe now have all the necessary ingredients to code Bayesian Logistic Regression up from scratch. While in practice we would usually want to rely on existing packages that have been properly tested, I often find it very educative and rewarding to program algorithms from the bottom up. You will see that Julia‚Äôs syntax so closely resembles the mathematical formulas we have seen above, that going from maths to code is incredibly easy. Seeing those formulas and algorithms then actually doing their magic is quite fun! The code chunk below, for example, shows the implementation of the loss function and its derivatives from @ref(eq:likeli) above. Take a moment to go through the code line-by-line and try to understand how it relates back to the equations in @ref(eq:likeli). Isn‚Äôt it amazing how closely the code resembles the actual equations?\n\nAside from the optimization routine this is essentially all there is to coding up Bayesian Logistic Regression from scratch in Julia Language. If you are curious to see the full source code in detail you can check out this interactive notebook. Now let us finally turn back to our synthetic data and see how Bayesian Logistic Regression can help us understand the uncertainty around our model predictions.\n\nDISCLAIMER ‚ùóÔ∏è\nI should mention that this is the first time I program in Julia, so for any Julia pros out there: please bear with me! Happy to hear your suggestions/comments."
  },
  {
    "objectID": "posts/bayesian-logit/index.html#the-estimates",
    "href": "posts/bayesian-logit/index.html#the-estimates",
    "title": "Bayesian Logistic Regression",
    "section": "The estimates",
    "text": "The estimates\nFigure @ref(fig:posterior) below shows the resulting posterior distribution for \\(w_2\\) and \\(w_3\\) at varying degrees of prior uncertainty \\(\\sigma\\). The constant \\(w_1\\) is held constant at the mode (\\(\\hat{w}_1\\)). The red dot indicates the MLE. Note how for the choice of \\(\\sigma\\rightarrow 0\\) the posterior is equal to the prior. This is intuitive since we have imposed that we have no uncertainty around our prior beliefs and hence no amount of new evidence can move us in any direction. Conversely, for \\(\\sigma \\rightarrow \\infty\\) the posterior distribution is centered around the unconstrained MLE: prior knowledge is very uncertain and hence the posterior is dominated by the likelihood of the data.\n\n\n\n\n\nPosterior distribution for \\(w_2\\) and \\(w_3\\) at varying degrees of prior uncertainty \\(\\sigma\\).\n\n\n\n\nWhat about the posterior predictive? The story is similar: since for \\(\\sigma\\rightarrow 0\\) the posterior is completely dominated by the zero-mean prior we have \\(p(y=1|\\mathbf{x},\\hat{\\mathbf{w}})=0.5\\) everywhere (top left panel in Figure @ref(fig:predictive)). As we gradually increase uncertainty around our prior the predictive posterior depends more and more on the data \\(\\mathcal{D}\\): uncertainty around predicted labels is high only in regions that are not populated by samples \\((y_n, \\mathbf{x}_n)\\). Not surprisingly, this effect is strongest for the MLE (\\(\\sigma\\rightarrow \\infty\\)) where we see some evidence of overfitting.\n\n\n\n\n\nPredictive posterior distribution at varying degrees of prior uncertainty \\(\\sigma\\)."
  },
  {
    "objectID": "posts/bayesian-logit/index.html#wrapping-up",
    "href": "posts/bayesian-logit/index.html#wrapping-up",
    "title": "Bayesian Logistic Regression",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this post we have seen how Bayesian Logistic Regression can be implemented from scratch in Julia language. The estimated posterior distribution over model parameters can be used to quantify uncertainty around coefficients and model predictions. I have argued that it is important to be transparent about model uncertainty to avoid being overly confident in estimates.\nThere are many more benefits associated with Bayesian (probabilistic) machine learning. Understanding where in the input domain our model exerts high uncertainty can for example be instrumental in labelling data: see for example Gal, Islam, and Ghahramani (2017) and follow-up works for an interesting application to active learning for image data. Similarly, there is a recent work that uses estimates of the posterior predictive in the context of algorithmic recourse (Schut et al. 2021). For a brief introduction to algorithmic recourse see one of my previous posts.\nAs a great reference for further reading about probabilistic machine learning I can highly recommend Murphy (2022). An electronic version of the book is currently freely available as a draft. Finally, remember that if you want to try yourself at the code, you can check out this interactive notebook."
  },
  {
    "objectID": "posts/bayesian-logit/index.html#references",
    "href": "posts/bayesian-logit/index.html#references",
    "title": "Bayesian Logistic Regression",
    "section": "References",
    "text": "References\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. springer.\n\n\nGal, Yarin, Riashat Islam, and Zoubin Ghahramani. 2017. ‚ÄúDeep Bayesian Active Learning with Image Data.‚Äù In International Conference on Machine Learning, 1183‚Äì92. PMLR.\n\n\nMurphy, Kevin P. 2012. Machine Learning: A Probabilistic Perspective. MIT press.\n\n\n‚Äî‚Äî‚Äî. 2022. Probabilistic Machine Learning: An Introduction. MIT Press.\n\n\nSchut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. ‚ÄúGenerating Interpretable Counterfactual Explanations by Implicit Minimisation of Epistemic and Aleatoric Uncertainties.‚Äù In International Conference on Artificial Intelligence and Statistics, 1756‚Äì64. PMLR."
  }
]