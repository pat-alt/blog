---
title: Conformal Prediction
subtitle: From scratch in Julia Language
author: Patrick Altmeyer
date: '2022-10-25'
categories:
  - probabilistic programming
  - uncertainty
  - Julia
description: >-
  A very gentle introduction to Conformal Prediction from the bottom up with examples in Julia language.
image: www/intro.gif
jupyter: julia-1.8
draft: false
---

```{julia}
using Pkg; Pkg.activate("posts/conformal-prediction")
```

<div class="intro-gif">
  <figure>
    <img src="www/intro.gif">
    <figcaption>A Bayesian Neural Network gradually learns.</figcaption>
  </figure>
</div>

A first crucial step towards building trustworthy AI systems is to be transparent about predictive uncertainty. Model parameters are random variables and their values are estimated from noisy data. That inherent stochasticity feeds through to model predictions and should to be addressed at the very least in order to avoid overconfidence in models. Beyond that obvious concern, it turns out that quantifying model uncertainty actually opens up a myriad of possibilities to improve up- and downstream modeling tasks like active learning and explainability. In Bayesian Active Learning, for example, uncertainty estimates are used to guide the search for new input samples, which can make ground-truthing tasks more efficient [@houlsby2011bayesian]. With respect to model performance in downstream tasks, uncertainty quantification can be used to improve model calibration and robustness [@lakshminarayanan2016simple]. 

In previous posts we have looked at how uncertainty can be quantified in the Bayesian context. Since in Bayesian modeling we are generally concerned with estimated posterior distributions, we get uncertainty estimates almost as a byproduct. This is great for all intends and purposes, but it hinges on assumptions about prior distributions. Personally, I have no quarrel with the idea of making prior distributional assumptions. On the contrary, I think the Bayesian framework formalizes the idea of integrating prior information in models and therefore provides a powerful toolkit for conducting science. Still, in some cases this requirement may be seen as too restrictive or we may simply lack prior information. 

## 👉 Enter: Conformal Prediction

Conformal Prediction (CP) promises to be a easy-to-understand, distribution-free and model-agnostic way to generate statistically rigorous uncertainty estimates. That's quite a mouthful, so let's break it down: firstly, as I will hopefully manage to illustrate in this post, the underlying concepts truly are fairly straight-forward to understand; secondly, CP indeed relies on only minimal distributional assumptions; thirdly, common procedures to generate conformal predictions really do apply almost universally to all supervised models, therefore making the framework very intriguing to the ML community; and, finally, CP does in fact come with a coverage guarantee that ensures that conformal prediction sets contain the true value with a user-chosen probability. For a formal proof of this *marginal coverage* property and a detailed introduction to the topic, I recommend @angelopoulos2021gentle. 

:::{.callout-note}
In what follows we will loosely treat the tutorial by @angelopoulos2021gentle and the general framework it sets as a reference. You are not expected to have read the paper, but I also won't reiterate any details here.
:::

## 💻 Conformal Prediction in Julia

In this section of this first short post on CP we will look at conformal prediction can be implemented in Julia to be compatible with any of the many supervised machine learning models available in [MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/): a beautiful, comprehensive machine learning framework funded by the [Alan Turing Institute](https://www.turing.ac.uk/) and the [New Zealand Strategic Science Investment Fund](https://www.mbie.govt.nz/science-and-technology/science-and-innovation/funding-information-and-opportunities/investment-funds/strategic-science-investment-fund/ssif-funded-programmes/university-of-auckland/). We will go through some basic usage examples employing a a new Julia package that I have been working on: [`ConformalPrediction.jl`](https://github.com/pat-alt/ConformalPrediction.jl).  

:::{.callout-note}
## [`ConformalPrediction.jl`](https://github.com/pat-alt/ConformalPrediction.jl)

`ConformalPrediction.jl` is a package for uncertainty quantification through conformal prediction for machine learning models trained in [MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/). At the time of writing it is still in its early stages of development. Contributions are very much welcome:

- [Documentation](https://www.paltmeyer.com/ConformalPrediction.jl/stable/)
- [Contributor's Guide](https://www.paltmeyer.com/ConformalPrediction.jl/stable/#Contribute)
:::

We consider a simple multi-class prediction problem. Let $(X_i, Y_i), \ i=1,...,n$ denote our feature-label pairs and let $\mu: \mathcal{X} \mapsto \mathcal{Y}$ denote the mapping from features to labels. For illustration purposes we will use the iris dataset. 

```{julia}
using MLJ
using Random
Random.seed!(123)

# Data:
X, y = make_moons(500; noise=0.15)
train, test = partition(eachindex(y), 0.8, shuffle=true)
```

We use a specific case of CP called *split conformal prediction* which can then be summarized as follows:^[In other places split conformal prediction is sometimes referred to as *inductive* conformal prediction.]

1. Partition the training into a proper training set and a separate calibration set: $\mathcal{D}_n=\mathcal{D}^{\text{train}} \cup \mathcal{D}^{\text{cali}}$.
2. Train the machine learning model on the proper training set: $\hat\mu_{i \in \mathcal{D}^{\text{train}}}(X_i,Y_i)$.
3. Compute nonconformity scores, $\mathcal{S}$, using the calibration data $\mathcal{D}^{\text{cali}}$ and the fitted model $\hat\mu_{i \in \mathcal{D}^{\text{train}}}$. 
4. For a user-specified desired coverage ratio $(1-\alpha)$ compute the corresponding quantile, $\hat{q}$, of the empirical distribution of nonconformity scores, $\mathcal{S}$.
5. For the given quantile and test sample $X_{\text{test}}$, form the corresponding conformal prediction set: 

$$
C(X_{\text{test}})=\{y:s(X_{\text{test}},y) \le \hat{q}\}
$$ {#eq-set}

The code below implements the simplest form of this procedure in Julia.  As a first important step, we begin by defining a concrete type `SimpleInductiveClassifier` that wraps a supervised model from [`MLJ.jl`] and reserves additional fields for a few hyperparameters. As a second step, we define the training procedure, which includes the data-splitting and calibration step. Finally, as a third step we implement the procedure in @eq-set to compute the conformal prediction set.

Now let's take this to our data. To illustrate the package functionality we will use the package API to construct our conformal predictor. We first define our atomic machine learning model following standard [`MLJ.jl`](https://alan-turing-institute.github.io/MLJ.jl/v0.18/) conventions. Using [`ConformalPrediction.jl`](https://github.com/pat-alt/ConformalPrediction.jl) we then wrap our atomic model into a conformal model using the standard API call `conformal_model`. To train and predict from our conformal model we can then rely on the conventional [`MLJ.jl`](https://alan-turing-institute.github.io/MLJ.jl/v0.18/) procedure again. The final predictions are set-valued. 

```{julia}
#| output: true

# Model:
KNNClassifier = @load KNNClassifier pkg=NearestNeighborModels
model = KNNClassifier(;K=50) 

# Training:
using ConformalPrediction
conf_model = conformal_model(model; coverage=.9)
mach = machine(conf_model, X, y)
fit!(mach, rows=train)

# Conformal Prediction:
Xtest = selectrows(X, first(test))
ytest = y[first(test)]
predict(mach, Xtest)[1]
```

While the softmax output remains unchanged for the `SimpleInductiveClassifier`, the size of the prediction set depends on the choice of coverage, $(1-\alpha)$. When specifying a coverage rate very close to one, the prediction set will typically include many (in some cases all) of the possible labels.

```{julia}
#| echo: false
coverage = 1.0
using Markdown
Markdown.parse("""
Below, for example, all three possible classes of iris plants are included in the prediction set when setting coverage the coverage rates equal to $coverage.
""")
```

This intuitive, since high coverage quite literally requires that the true label is covered by the prediction set with high probability.

```{julia}
#| output: true

conf_model = conformal_model(model; coverage=coverage)
mach = machine(conf_model, X, y)
fit!(mach, rows=train)

# Conformal Prediction:
Xtest = (x1=[1],x2=[0])
predict(mach, Xtest)[1]
```

Conversely, for low coverage rates, prediction sets can also be empty. 

```{julia}
#| echo: false
coverage = .1
using Markdown
Markdown.parse("""
For a choice of $coverage, for example, the prediction set for our test sample is empty.
""")
```

This is a bit difficult to think about intuitively and I have not yet come across a satisfactory, intuitive interpretation.^[Any thoughts/comments welcome!]

```{julia}
#| output: true

conf_model = conformal_model(model; coverage=coverage)
mach = machine(conf_model, X, y)
fit!(mach, rows=train)

# Conformal Prediction:
Xtest = (x1=[1],x2=[0])
predict(mach, Xtest)[1]
```

```{julia}
#| echo: false
using Plots

function contourf_cp(mach::Machine, x1_range, x2_range; type=:set_size)
    set_size = []
    proba = []
    for x2 in x2_range, x1 in x1_range
        Xnew = (x1 = [x1], x2 = [x2])
        p̂ = predict(mach, Xnew)[1]
        # Set size:
        z = ismissing(p̂) ? 0 : sum(pdf.(p̂, p̂.decoder.classes) .> 0)
        push!(set_size, z)
        # Probability:
        p = ismissing(p̂) ? p̂ : pdf.(p̂, 1)
        push!(proba, p)
    end
    if type == :set_size
        plt = contourf(x1_range, x2_range, set_size, clim=(0,2), c=cgrad(:blues, 3, categorical = true), title="Set size")
    elseif type == :proba
        plt = contourf(x1_range, x2_range, proba, c=:thermal, title="Softmax")
    end
    return plt
end
```

@fig-anim illustrates the effect of the chosen coverage rate on the 

```{julia}
#| output: true
#| label: fig-anim
#| fig-cap: "The effect of the coverage rate on the conformal prediction set. Softmax probabilities are shown on the left. The size of the prediction set is shown on the right."

# Setup
coverages = range(0.2,1.0,length=5)
n = 100
x1_range = range(extrema(X.x1)...,length=n)
x2_range = range(extrema(X.x2)...,length=n)
anim = @animate for coverage in coverages
    conf_model = conformal_model(model; coverage=coverage)
    mach = machine(conf_model, X, y)
    fit!(mach, rows=train)
    p1 = contourf_cp(mach, x1_range, x2_range; type=:proba)
    scatter!(p1, X.x1, X.x2, group=y, ms=2, msw=0, alpha=0.5)
    scatter!(p1, Xtest.x1, Xtest.x2, ms=5, c=:red, label="Xtest")
    p2 = contourf_cp(mach, x1_range, x2_range; type=:set_size)
    scatter!(p2, X.x1, X.x2, group=y, ms=2, msw=0, alpha=0.5)
    scatter!(p2, Xtest.x1, Xtest.x2, ms=5, c=:red, label="Xtest")
    plot(p1, p2, plot_title="(1-α)=$(round(coverage,digits=2))", size=(800,250))
end

gif(anim, fps=0.5)
```

```{julia}
#| echo: false
#| eval: false
# Setup
coverages = range(0.75,1.0,length=25)
n = 100
x1_range = range(extrema(X.x1)...,length=n)
x2_range = range(extrema(X.x2)...,length=n)
anim = @animate for coverage in coverages
    conf_model = conformal_model(model; coverage=coverage)
    mach = machine(conf_model, X, y)
    fit!(mach, rows=train)
    plt = contourf_cp(mach, x1_range, x2_range; type=:proba)
    plot(plt, axis=nothing, size=(500,500), background_color=:transparent, title="", clegend=false)
end
gif(anim, "posts/conformal-prediction/www/intro.gif", fps=5)
```


## 🏃‍♀️ TL;DR

Implementing CP directly on top of an existing, powerful machine learning toolkit demonstrates the potential usefulness of this framework to the ML community. 

## 📦 Related Packages

## 📚 Further Resources

Chances are that you have already come across the Awesome Conformal Prediction [repo](https://github.com/valeman/awesome-conformal-prediction): @manokhin2022awesome provides a comprehensive, up-to-date overview of resources related to the conformal prediction. Among the listed articles you will also find @angelopoulos2021gentle, which inspired much of this post. 



