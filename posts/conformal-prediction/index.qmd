---
title: Conformal Prediction
subtitle: From scratch in Julia Language
author: Patrick Altmeyer
date: '2022-10-25'
categories:
  - probabilistic programming
  - uncertainty
  - Julia
description: >-
  A very gentle introduction to Conformal Prediction from the bottom up with examples in Julia language.
image: www/intro.gif
jupyter: julia-1.8
draft: false
---

```{julia}
using Pkg; Pkg.activate("posts/conformal-prediction")
```

<div class="intro-gif">
  <figure>
    <img src="www/intro.gif">
    <figcaption>A Bayesian Neural Network gradually learns.</figcaption>
  </figure>
</div>

A first crucial step towards building trustworthy AI systems is to be transparent about predictive uncertainty. Model parameters are random variables and their values are estimated from noisy data. That inherent stochasticity feeds through to model predictions and should to be addressed at the very least in order to avoid overconfidence in models. Beyond that obvious concern, it turns out that quantifying model uncertainty actually opens up a myriad of possibilities to improve up- and downstream modeling tasks like active learning and explainability. In Bayesian Active Learning, for example, uncertainty estimates are used to guide the search for new input samples, which can make ground-truthing tasks more efficient [@houlsby2011bayesian]. With respect to model performance in downstream tasks, uncertainty quantification can be used to improve model calibration and robustness [@lakshminarayanan2016simple]. 

In previous posts we have looked at how uncertainty can be quantified in the Bayesian context. Since in Bayesian modeling we are generally concerned with estimated posterior distributions, we get uncertainty estimates almost as a byproduct. This is great for all intends and purposes, but it hinges on assumptions about prior distributions. Personally, I have no quarrel with the idea of making prior distributional assumptions. On the contrary, I think the Bayesian framework formalizes the idea of integrating prior information in models and therefore provides a powerful toolkit for conducting science. Still, in some cases this requirement may be seen as too restrictive or we may simply lack prior information. 

## üëâ Enter: Conformal Prediction

Conformal Prediction (CP) promises to be a easy-to-understand, distribution-free and model-agnostic way to generate statistically rigorous uncertainty estimates. That's quite a mouthful, so let's break it down: firstly, as I will hopefully manage to illustrate in this post, the underlying concepts truly are fairly straight-forward to understand; secondly, CP indeed relies on only minimal distributional assumptions; thirdly, common procedures to generate conformal predictions really do apply almost universally to all supervised models, therefore making the framework very intriguing to the ML community; and, finally, CP does in fact come with a coverage guarantee that ensures that conformal prediction sets contain the true value with a user-chosen probability. For a formal proof of this *marginal coverage* property and a detailed introduction to the topic, I recommend @angelopoulos2021gentle. 

:::{.callout-note}
In what follows we will loosely treat the tutorial by @angelopoulos2021gentle and the general framework it sets as a reference. You are not expected to have read the paper, but I also won't reiterate any details here.
:::

## üíª Conformal Prediction in Julia

In this section of this first post on CP we will see how *split conformal prediction* (SCP) can be implemented in Julia to be compatible with any of the many supervised machine learning models available in [MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/): a beautiful, comprehensive machine learning framework funded by the [Alan Turing Institute](https://www.turing.ac.uk/) and the [New Zealand Strategic Science Investment Fund](https://www.mbie.govt.nz/science-and-technology/science-and-innovation/funding-information-and-opportunities/investment-funds/strategic-science-investment-fund/ssif-funded-programmes/university-of-auckland/). 

We consider a simple multi-class prediction problem. Let $(X_i, Y_i), \ i=1,...,n$ denote our feature-label pairs and let $\mu: \mathcal{X} \mapsto \mathcal{Y}$ denote the mapping from features to labels. For illustration purposes we will use the iris dataset. 

```{julia}
using MLJ
using Random
Random.seed!(42)

# Data:
X, y = @load_iris
train, test = partition(eachindex(y), 0.8, shuffle=true)
```

Split conformal prediction (also elsewhere referred to as *inductive* conformal prediction) can then be summarized as follows:

1. Partition the training into a proper training set and a separate calibration set: $\mathcal{D}_n=\mathcal{D}^{\text{train}} \cup \mathcal{D}^{\text{cali}}$.
2. Train the machine learning model on the proper training set: $\hat\mu_{i \in \mathcal{D}^{\text{train}}}(X_i,Y_i)$.
3. Compute nonconformity scores, $\mathcal{S}$, using the calibration data $\mathcal{D}^{\text{cali}}$ and the fitted model $\hat\mu_{i \in \mathcal{D}^{\text{train}}}$. 
4. For a user-specified desired coverage ratio $(1-\alpha)$ compute the corresponding quantile, $\hat{q}$, of the empirical distribution of nonconformity scores, $\mathcal{S}$.
5. For the given quantile and test sample $X_{\text{test}}$, form the corresponding conformal prediction set: 

$$
C(X_{\text{test}})=\{y:s(X_{\text{test}},y) \le \hat{q}\}
$$ {#eq-set}

The code below implements the simplest form of this procedure in Julia. It is lifted from the source code of [`ConformalPrediction.jl`]: a package for CP in Julia that I have been working on. As a first important step, we begin by defining a concrete type `SimpleInductiveClassifier` that wraps a supervised model from [`MLJ.jl`] and reserves additional fields for a few hyperparameters. As a second step, we define the training procedure, which includes the data-splitting and calibration step. Finally, as a third step we implement the procedure in @eq-set to compute the conformal prediction set.

<script src="https://gist.github.com/pat-alt/6f0dfeb471dca927387a46f93ab5ee1b.js"></script>

Now let's take this to our data. To illustrate the package functionality we will use the package API to construct our conformal predictor. We first define our atomic machine learning model following standard [`MLJ.jl`](https://alan-turing-institute.github.io/MLJ.jl/v0.18/) conventions. Using [`ConformalPrediction.jl`](https://github.com/pat-alt/ConformalPrediction.jl) we then wrap our atomic model into a conformal model using the standard API call `conformal_model`. To train and predict from our conformal model we can then rely on the conventional [`MLJ.jl`](https://alan-turing-institute.github.io/MLJ.jl/v0.18/) procedure again. The final predictions are set-valued.

```{julia}
#| output: true

# Model:
EvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees
model = EvoTreeClassifier() 

# Training:
using ConformalPrediction
conf_model = conformal_model(model; coverage=.9)
mach = machine(conf_model, X, y)
fit!(mach, rows=train)

# Conformal Prediction:
Xtest = selectrows(X, first(test))
ytest = y[first(test)]
predict(mach, Xtest)[1]
```

```{julia}
#| output: true

conf_model = conformal_model(model; coverage=1.0-1e-10)
mach = machine(conf_model, X, y)
fit!(mach, rows=train)

# Conformal Prediction:
Xtest = selectrows(X, first(test))
ytest = y[first(test)]
predict(mach, Xtest)[1]
```

```{julia}
#| output: true

conf_model = conformal_model(model; coverage=1e-10)
mach = machine(conf_model, X, y)
fit!(mach, rows=train)

# Conformal Prediction:
Xtest = selectrows(X, first(test))
ytest = y[first(test)]
predict(mach, Xtest)[1]
```


## üèÉ‚Äç‚ôÄÔ∏è TL;DR

Implementing CP directly on top of an existing, powerful machine learning toolkit demonstrates the potential usefulness of this framework to the ML community. 

## üì¶ Related Packages

## üìö Further Resources

Chances are that you have already come across the Awesome Conformal Prediction [repo](https://github.com/valeman/awesome-conformal-prediction): @manokhin2022awesome provides a comprehensive, up-to-date overview of resources related to the conformal prediction. Among the listed articles you will also find @angelopoulos2021gentle, which inspired much of this post. 



