{
  "hash": "86c9df0309f81be61de8da1e26e97bc7",
  "result": {
    "markdown": "---\ntitle: Conformal Prediction\nsubtitle: From scratch in Julia Language\nauthor: Patrick Altmeyer\ndate: '2022-10-25'\ncategories:\n  - probabilistic programming\n  - uncertainty\n  - Julia\ndescription: A very gentle introduction to Conformal Prediction from the bottom up with examples in Julia language.\nimage: www/intro.gif\nexecute:\n  eval: false\ndraft: true\n---\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing Pkg; Pkg.activate(\"posts/conformal-prediction\")\n```\n:::\n\n\n<div class=\"intro-gif\">\n  <figure>\n    <img src=\"www/intro.gif\">\n    <figcaption>A Bayesian Neural Network gradually learns.</figcaption>\n  </figure>\n</div>\n\nA first crucial step towards building trustworthy AI systems is to be transparent about predictive uncertainty. Model parameters are random variables and their values are estimated from noisy data. That inherent stochasticity feeds through to model predictions and should to be addressed at the very least in order to avoid overconfidence in models. Beyond that obvious concern, it turns out that quantifying model uncertainty actually opens up a myriad of possibilities to improve up- and downstream modeling tasks like active learning and explainability. In Bayesian Active Learning, for example, uncertainty estimates are used to guide the search for new input samples, which can make ground-truthing tasks more efficient [@houlsby2011bayesian]. With respect to model performance in downstream tasks, uncertainty quantification can be used to improve model calibration and robustness [@lakshminarayanan2016simple]. \n\nIn previous posts we have looked at how uncertainty can be quantified in the Bayesian context. Since in Bayesian modeling we are generally concerned with estimated posterior distributions, we get uncertainty estimates almost as a byproduct. This is great for all intends and purposes, but it hinges on assumptions about prior distributions. Personally, I have no quarrel with the idea of making prior distributional assumptions. On the contrary, I think the Bayesian framework formalizes the idea of integrating prior information in models and therefore provides a powerful toolkit for conducting science. Still, in some cases this requirement may be seen as too restrictive or we may simply lack prior information. \n\n## 👉 Enter: Conformal Prediction\n\nConformal Prediction (CP) promises to be a easy-to-understand, distribution-free and model-agnostic way to generate statistically rigorous uncertainty estimates. That's quite a mouthful, so let's break it down: firstly, as I will hopefully manage to illustrate in this post, the underlying concepts truly are fairly straight-forward to understand; secondly, CP indeed relies on only minimal distributional assumptions; thirdly, common procedures to generate conformal predictions really do apply almost universally to all supervised models, therefore making the framework very intriguing to the ML community; and, finally, CP does in fact come with a coverage guarantee that ensures that conformal prediction sets contain the true value with a user-chosen probability. For a formal proof of this *marginal coverage* property and a detailed introduction to the topic, I recommend @angelopoulos2021gentle. \n\n:::{.callout-note}\nIn what follows we will loosely treat the tutorial by @angelopoulos2021gentle and the general framework it sets as a reference. You are not expected to have read the paper, but I also won't reiterate any details here.\n:::\n\n## 🟣🔴🟢 Conformal Prediction in Julia\n\nIn this section of this first post on CP we will see how *split conformal prediction* (SCP) can be implemented in Julia to be compatible with any of the many supervised machine learning models available in [MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/): a beautiful, comprehensive machine learning framework funded by the [Alan Turing Institute](https://www.turing.ac.uk/) and the [New Zealand Strategic Science Investment Fund](https://www.mbie.govt.nz/science-and-technology/science-and-innovation/funding-information-and-opportunities/investment-funds/strategic-science-investment-fund/ssif-funded-programmes/university-of-auckland/). \n\nWe consider a simple multi-class prediction problem. Let $(X_i, Y_i), \\ i=1,...,n$ denote our feature-label pairs and let $\\mu: \\mathcal{X} \\mapsto \\mathcal{Y}$ denote the mapping from features to labels. A corresponding toy dataset is shown in @fig-data. \n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing MLJ\n\n# Data:\nX, y = @load_iris\nreplace!(y, \"setosa\" => \"🟣\", \"versicolor\" => \"🔴\", \"virginica\" => \"🟢\")\ntrain, test = partition(eachindex(y), 0.8, shuffle=true)\n```\n:::\n\n\nSplit conformal prediction (also elsewhere referred to as *inductive* conformal prediction) can then be summarized as follows:\n\n1. Partition the training into a proper training set and a separate calibration set: $\\mathcal{D}_n=\\mathcal{D}^{\\text{train}} \\cup \\mathcal{D}^{\\text{calibration}}$.\n2. Train the machine learning model on the proper training set: $\\hat\\mu_{i \\in \\mathcal{D}^{\\text{train}}}(X_i,Y_i)$.\n3. Compute nonconformity scores, $\\mathcal{S}$, using the calibration data $\\mathcal{D}^{\\text{calibration}}$ and the fitted model $\\hat\\mu_{i \\in \\mathcal{D}^{\\text{train}}}$. \n4. For a user-specified desired coverage ratio $(1-\\alpha)$ compute the corresponding quantile, $\\hat{q}$, of the empirical distribution of nonconformity scores, $\\mathcal{S}$.\n5. For the given quantile and test sample $X_{\\text{test}}$, form the corresponding conformal prediction set: \n\n$$\nC(X_{\\text{test}})=\\{y:s(X_{\\text{test}},y) \\le \\hat{q}\\}\n$$ (#eq)\n\nThe code below implements the simplest form of this procedure in Julia. It is lifted from the source code of [`ConformalPrediction.jl`]: a package for CP in Julia that I have been working on. As a first important step, we begin by defining a concrete type `SimpleInductiveClassifier` that wraps a supervised model from [`MLJ.jl`] and reserves additional fields for a few hyperparameters. As a second step, we define the training procedure, which includes the data-splitting and calibration step. Finally, as a third step we define the way prediction sets are formed based on the estimated nonconformity scores.\n\n```{.julia}\n# Simple\n\"The `SimpleInductiveClassifier` is the simplest approach to Inductive Conformal Classification. Contrary to the [`NaiveClassifier`](@ref) it computes nonconformity scores using a designated calibration dataset.\"\nmutable struct SimpleInductiveClassifier{Model <: Supervised} <: ConformalSet\n    model::Model\n    coverage::AbstractFloat\n    scores::Union{Nothing,AbstractArray}\n    heuristic::Function\n    train_ratio::AbstractFloat\nend\n\nfunction SimpleInductiveClassifier(model::Supervised; coverage::AbstractFloat=0.95, heuristic::Function=f(y, ŷ)=1.0-ŷ, train_ratio::AbstractFloat=0.5)\n    return SimpleInductiveClassifier(model, coverage, nothing, heuristic, train_ratio)\nend\n\n@doc raw\"\"\"\n    MMI.fit(conf_model::SimpleInductiveClassifier, verbosity, X, y)\nFor the [`SimpleInductiveClassifier`](@ref) nonconformity scores are computed as follows:\n``\nS_i^{\\text{CAL}} = s(X_i, Y_i) = h(\\hat\\mu(X_i), Y_i), \\ i \\in \\mathcal{D}_{\\text{calibration}}\n``\nA typical choice for the heuristic function is ``h(\\hat\\mu(X_i), Y_i)=1-\\hat\\mu(X_i)_{Y_i}`` where ``\\hat\\mu(X_i)_{Y_i}`` denotes the softmax output of the true class and ``\\hat\\mu`` denotes the model fitted on training data ``\\mathcal{D}_{\\text{train}}``. The simple approach only takes the softmax probability of the true label into account.\n\"\"\"\nfunction MMI.fit(conf_model::SimpleInductiveClassifier, verbosity, X, y)\n    \n    # Data Splitting:\n    train, calibration = partition(eachindex(y), conf_model.train_ratio)\n    Xtrain = MLJ.matrix(X)[train,:]\n    ytrain = y[train]\n    Xcal = MLJ.matrix(X)[calibration,:]\n    ycal = y[calibration]\n\n    # Training: \n    fitresult, cache, report = MMI.fit(conf_model.model, verbosity, MMI.reformat(conf_model.model, Xtrain, ytrain)...)\n\n    # Nonconformity Scores:\n    ŷ = pdf.(MMI.predict(conf_model.model, fitresult, Xcal), ycal)\n    conf_model.scores = @.(conf_model.heuristic(ycal, ŷ))\n\n    return (fitresult, cache, report)\nend\n\n@doc raw\"\"\"\n    MMI.predict(conf_model::SimpleInductiveClassifier, fitresult, Xnew)\nFor the [`SimpleInductiveClassifier`](@ref) prediction sets are computed as follows,\n``\n\\hat{C}_{n,\\alpha}(X_{n+1}) = \\left\\{y: s(X_{n+1},y) \\le \\hat{q}_{n, \\alpha}^{+} \\{S_i^{\\text{CAL}}\\} \\right\\}, \\ i \\in \\mathcal{D}_{\\text{calibration}}\n``\nwhere ``\\mathcal{D}_{\\text{calibration}}`` denotes the designated calibration data.\n\"\"\"\nfunction MMI.predict(conf_model::SimpleInductiveClassifier, fitresult, Xnew)\n    p̂ = MMI.predict(conf_model.model, fitresult, MMI.reformat(conf_model.model, Xnew)...)\n    L = p̂.decoder.classes\n    ŷ = pdf(p̂, L)\n    v = conf_model.scores\n    q̂ = Statistics.quantile(v, conf_model.coverage)\n    ŷ = map(x -> collect(key => 1.0-val <= q̂ ? val : missing for (key,val) in zip(L,x)),eachrow(ŷ))\n    return ŷ\nend\n```\n\nNow let's take this to our data. To illustrate the package functionality we will use the package API to construct our conformal predictor. We first define our atomic machine learning model following standard [`MLJ.jl`] conventions. Using [`ConformalPrediction.jl`] we then wrap our atomic model into a conformal model using the standard API call `conformal_model`. To train and predict from our conformal model we can then rely on the conventional [`MLJ.jl`] procedure again. The final predictions are set-valued.\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\n# Model:\nEvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\nmodel = EvoTreeClassifier() \n\n# Training:\nusing ConformalPrediction\nconf_model = conformal_model(model)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n\n# Conformal Prediction:\nXtest = selectrows(X, first(test))\nytest = y[first(test)]\npredict(mach, Xtest)[1]\n```\n:::\n\n\n## 🏃‍♀️ TL;DR\n\nImplementing CP directly on top of an existing, powerful machine learning toolkit demonstrates the potential usefulness of this framework to the ML community. \n\n## 📦 Related Packages\n\n## 📚 Further Resources\n\nChances are that you have already come across the Awesome Conformal Prediction [repo](https://github.com/valeman/awesome-conformal-prediction): @manokhin2022awesome provides a comprehensive, up-to-date overview of resources related to the conformal prediction. Among the listed articles you will also find @angelopoulos2021gentle, which inspired much of this post. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}